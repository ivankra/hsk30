{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a039b5b-ecae-4f16-ad40-0509f4faff49",
   "metadata": {},
   "source": [
    "# HSK 3.0 wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60947e-b815-4f92-9bf7-2408d208f345",
   "metadata": {},
   "source": [
    "**Primary sources**\n",
    "  * http://www.moe.gov.cn/jyb_xwfb/gzdt_gzdt/s5987/202103/t20210329_523304.html\n",
    "  * http://www.moe.gov.cn/jyb_xwfb/gzdt_gzdt/s5987/202103/W020210329527301787356.pdf\n",
    "    * Scanned document without OCR layer\n",
    "    * Error: `1856 火药` misindexed in .pdf, should be `1836 火药`\n",
    "    * SHA256 e451fdf0899d9267bbd122db66e7e75bdd2851ad1e6732e47b6e960290d73a63\n",
    "  * https://www.chinesetest.cn/standardsAction.do?means=standardInfo\n",
    "    * Parseable pinyin and part of speech for most terms available\n",
    "    * Indexing doesn't match .pdf, fuzzy sorted by pinyin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a6d96-e634-4cda-adb4-8b80e3fd329c",
   "metadata": {},
   "source": [
    "**Third party sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b51e4-4faf-4a09-94cc-5ebc674de0e7",
   "metadata": {},
   "source": [
    "**https://github.com/elkmovie/hsk30**\n",
    "* OCRed list by Pleco, only characters, no pinyin\n",
    "* https://www.plecoforums.com/threads/hsk-3-0-flashcards.6706/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a24cc0-f300-4b60-a5ba-0e5e8dbc5016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4a8cfdc3a8fa85ced837c71aa187454c4d0ee36a1f70765655a93acb6a463af3  downloads/pleco.txt\n",
      "4c779e47844147bc5aac7ff835e5ee8e2f312125314153e9c5e844501ed79897  downloads/pleco-charlist.txt\n"
     ]
    }
   ],
   "source": [
    "!pip install -q opencc\n",
    "\n",
    "import os, io, json, re\n",
    "import opencc\n",
    "import pandas as pd\n",
    "\n",
    "![ -f downloads/pleco.txt ] || curl -s -o downloads/pleco.txt https://raw.githubusercontent.com/elkmovie/hsk30/main/wordlist.txt\n",
    "![ -f downloads/pleco-charlist.txt ] || curl -s -o downloads/pleco-charlist.txt https://raw.githubusercontent.com/elkmovie/hsk30/main/charlist.txt\n",
    "!sha256sum downloads/pleco.txt downloads/pleco-charlist.txt\n",
    "\n",
    "# Parse and convert to .csv\n",
    "\n",
    "level = 0\n",
    "idx = 0\n",
    "rows = []\n",
    "\n",
    "for line in open('downloads/pleco.txt'):\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith('#'):\n",
    "        continue\n",
    "    if '级词汇表' in line:\n",
    "        level += 1\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx += 1\n",
    "        assert ' ' in line\n",
    "        assert line.split()[0] == str(idx) or line == '1856 火药', (idx,line)  # typo in original .pdf\n",
    "        rows.append({\n",
    "            'ID': f'L{level}-{idx:04d}',\n",
    "            'Level': str(level) if level < 7 else '7-9',\n",
    "            'No': line.split()[0],  # note: has 1856 火药\n",
    "            'OCR': line[len(str(idx)):].strip()\n",
    "        })\n",
    "\n",
    "pleco_df = pd.DataFrame(rows)\n",
    "pleco_df.to_csv('downloads/pleco.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8d99d-199f-49fc-8df2-3ac771de6341",
   "metadata": {},
   "source": [
    "**https://github.com/shawkynasr/HSK-official-Query-System/**\n",
    "* Scrape from [HSK website](https://www.chinesetest.cn/standardsAction.do?means=getStandardWordsList&leves=&words=&pinyin=&words_type=&pager.offset=0)?\n",
    "* Pinyin and POS for most terms\n",
    "* Two files, 2022 version is more accurate:\n",
    "  * corrected error: 四级,会,huìlǜ,名 -> 四级,汇率,huìlǜ,名\n",
    "  * corrected error: 一级,妈妈,māma∣mā,名 -> 一级,妈妈∣妈,māma∣mā,名\n",
    "  * dropped brackets that contained POS; still has brackets for variants (\"有（一）些\") and examples for prefix/suffix entries (\"们（朋友们）\")\n",
    "  * some off by one indexing error fixed\n",
    "* Grammar points\n",
    "* Character list seems wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed24b31-5d83-4b14-85ee-1f53484c3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f79d832ffe2a4015d5a24d6aa9c6f734e90053a02456fa0fc37eeefb65c630cc  downloads/shawkynasr2021.csv\n",
      "e4fd1e046f3fb94c2dd7ca7a34ae671230ca173cca50af5ff7d2913399150038  downloads/shawkynasr2022.csv\n",
      "f5d2b25e889ce9b54034612dcf16500d53f6168086d3642b0bb3749f0644d783  downloads/shawkynasr-grammar.csv\n",
      "2a2393d7896e4d43959a0a25f931286f00d73385655252fba0fa7f32f4534fee  downloads/shawkynasr-hanzi.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -e\n",
    "\n",
    "[[ -d ../downloads/hsk30 && ! -d downloads ]] && ln -s ../downloads/hsk30 downloads\n",
    "[[ -d downloads ]] || mkdir -p downloads\n",
    "\n",
    "for url in \\\n",
    "    downloads/shawkynasr2021.csv::https://raw.githubusercontent.com/shawkynasr/HSK-official-Query-System/main/%E8%AF%8D%E6%B1%87.csv \\\n",
    "    downloads/shawkynasr2022.csv::https://raw.githubusercontent.com/shawkynasr/HSK-official-Query-System/main/%E8%AF%8D%E6%B1%87%202022.csv \\\n",
    "    downloads/shawkynasr-grammar.csv::https://raw.githubusercontent.com/shawkynasr/HSK-official-Query-System/main/%E8%AF%AD%E6%B3%95.csv \\\n",
    "    downloads/shawkynasr-hanzi.csv::https://raw.githubusercontent.com/shawkynasr/HSK-official-Query-System/main/%E6%B1%89%E5%AD%97.csv; do\n",
    "  filename=\"${url%::*}\"\n",
    "  url=\"${url#*::}\"\n",
    "  [[ -f \"$filename\" ]] || curl -s -o \"$filename\" \"$url\"\n",
    "done\n",
    "\n",
    "sha256sum downloads/shawkynasr*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68463f87-0f06-4731-9e72-e620ae92f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh2021_df = pd.read_csv('downloads/shawkynasr2021.csv')\n",
    "sh2022_df = pd.read_csv('downloads/shawkynasr2022.csv')\n",
    "\n",
    "# Ignoring brackets and some formatting diffs, lists are same as pleco's:\n",
    "def norm(c):\n",
    "    c = c.str.replace('（.*', '', regex=True)\n",
    "    for x,y in ['∣｜', '1¹', '2²']:\n",
    "        c = c.str.replace(x, y)\n",
    "    return c.sort_values()\n",
    "assert list(sh2022_df['No.']) == list(range(1, 11093))\n",
    "assert set(norm(sh2022_df.iloc[:, 2])) == set(norm(pleco_df.OCR))\n",
    "assert len(set(norm(pleco_df.OCR)) ^ set(norm(sh2021_df.iloc[:, 2]))) <= 3  # {'妈妈', '妈妈｜妈', '汇率'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86742ac1-08ab-4f6e-ac87-3d3639cfc1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grammar points\n",
    "df = pd.read_csv('downloads/shawkynasr-grammar.csv').rename(columns={\n",
    "    'No.': 'No',\n",
    "    '级别': 'Level',\n",
    "    '语法项目': 'Group',\n",
    "    '类别': 'Category',\n",
    "    '细目': 'Details',\n",
    "    '语法内容': 'Content',\n",
    "})\n",
    "df['Level'] = df['Level'].map({'一级':'1', '三级':'3', '二级':'2', '五级':'5', '六级':'6', '四级':'4', '高等':'7-9'})\n",
    "df.to_csv('hsk30-grammar.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf16dc-ce17-4461-84a9-89635d029329",
   "metadata": {},
   "source": [
    "**https://github.com/andycburke/HSK-3.0-Word-List/**\n",
    "* Alternatively OCRed list\n",
    "* Obsolete/abandonded: all corrections now already incorporated in pleco's list, still has one unfixed error: 人手 (https://github.com/andycburke/HSK-3.0-Word-List/pull/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a1324b-b6a0-4176-8f69-b21320774bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f downloads/andycburke.csv ] || curl -s -o downloads/andycburke.csv https://raw.githubusercontent.com/andycburke/HSK-3.0-Word-List/main/HSK-3.0-Word-List.csv\n",
    "df = pd.read_csv('downloads/andycburke.csv', dtype='str').fillna('')\n",
    "assert list(df.HSK_3_0_Level) == list(pleco_df.Level)\n",
    "assert list(df.HSK_3_0_No) == list(pleco_df.No)  # has 1856 火药\n",
    "assert len(df[df.OCR != pleco_df.OCR]) <= 2\n",
    "# L7-3498: correct 入手 rùshǒu    (pleco)\n",
    "# L5-0101: correct 称²（动) chēng (pleco)\n",
    "#print(df[df.OCR != pleco_df.OCR], '\\npleco:\\n', pleco_df[df.OCR != pleco_df.OCR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29609a9-f608-406e-bf9a-81c5994dc52c",
   "metadata": {},
   "source": [
    "**https://github.com/krmanik/HSK-3.0-words-list**\n",
    "  * Based on pleco's list, some confusing old files in repo\n",
    "  * Pinyin derived from hanzi, so has extra pinyin; entries with same hanzi merged together\n",
    "  * Has traditional terms, but very low quality in ambiguous cases - obviously not checked, even basic characters.\n",
    "  * Variants missing\n",
    "  * 入手 error from pleco's list not fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c629451-8d84-4507-93cd-4e8c327954f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "krmanik = ['\\t'.join(['Level', 'Simplified', 'Traditional', 'Pinyin', 'Zhuyin', 'Audio', 'Sim'])]\n",
    "\n",
    "for lv in ['1', '2', '3', '4', '5', '6', '7-9']:\n",
    "    if not os.path.exists(f'downloads/krmanik{lv}.tsv'):\n",
    "        !curl -o \"downloads/krmanik{lv}.tsv\" \"https://raw.githubusercontent.com/krmanik/HSK-3.0-words-list/main/HSK%20list%20with%20meaning/Anki%20xiehanzi__HSK%20{lv}.tsv\"\n",
    "    for line in open(f'downloads/krmanik{lv}.tsv'):\n",
    "        krmanik.append(lv + '\\t' + line.rstrip('\\r\\n'))\n",
    "\n",
    "df = pd.read_csv(io.StringIO('\\n'.join(krmanik)), sep='\\t')\n",
    "df['Pinyin'] = df.Pinyin.str.replace('</?span[^>]*>', '', regex=True)\n",
    "df.to_csv('downloads/krmanik.csv', index=False)\n",
    "\n",
    "if 0 and os.path.exists('hsk30-expanded.csv'):\n",
    "    #hdf = pd.read_csv('hsk30.csv', dtype='str').fillna('')\n",
    "    hdf = pd.read_csv('hsk30-expanded.csv', dtype='str').fillna('')[lambda X: X.Example != '1']\n",
    "    for lv in sorted(set(df.Level)):\n",
    "        miss = set(hdf[lambda X: (X.Level == lv)].Simplified) - set(df[df.Level == lv].Simplified)\n",
    "        extra = set(df[df.Level == lv].Simplified) - set(hdf[lambda X: X.Level == lv].Simplified)\n",
    "        print('L%s missing: %s, extra: %s' % (lv, miss, extra))\n",
    "\n",
    "    trads = {}\n",
    "    for row in hdf.itertuples():\n",
    "        trads.setdefault(row.Simplified, []).extend(row.Traditional.split('|'))\n",
    "    k = 0\n",
    "    for row in df.itertuples():\n",
    "        if row.Traditional not in trads.get(row.Simplified, []):\n",
    "            k += 1\n",
    "            print(k, row.Simplified, row.Traditional, 'not in', set(trads.get(row.Simplified, [])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cededae-d6a9-42f2-9cd1-fc8b3e8a02b1",
   "metadata": {},
   "source": [
    "## Merge and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf543e6-39c0-4b53-9b4f-f6cfe238472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data source: a scrape of vocab from https://www.chinesetest.cn/standardsAction.do?means=getStandardWordsList&leves=&words=&pinyin=&words_type=&pager.offset=0\n",
    "# https://raw.githubusercontent.com/shawkynasr/HSK-official-Query-System/main/%E8%AF%8D%E6%B1%87%202022.csv\n",
    "sh_df = pd.read_csv('downloads/shawkynasr2022.csv', dtype='str').fillna('').rename(columns={\n",
    "    'No.': 'No',\n",
    "    '词语': 'Hanzi',\n",
    "    '级别': 'Level',\n",
    "    '拼音': 'Pinyin',\n",
    "    '词性': 'POS',\n",
    "})\n",
    "sh_df['Level'] = sh_df['Level'].map({'一级':1, '三级':3, '二级':2, '五级':5, '六级':6, '四级':4, '高等':7})\n",
    "\n",
    "# Auxiliary source (for verification and term indexes in original .pdf): pleco's OCRed list\n",
    "# https://raw.githubusercontent.com/elkmovie/hsk30/main/wordlist.txt\n",
    "pleco_df = pd.read_csv('downloads/pleco.csv', dtype='str').fillna('').set_index('ID')\n",
    "level_ocr_to_id = {}\n",
    "for row in pleco_df.reset_index().itertuples():\n",
    "    key = (int(row.Level[0]), row.OCR)\n",
    "    level_ocr_to_id.setdefault(key, []).append(row.ID)\n",
    "\n",
    "# Ambiguous entries crossreferenced against original pdf\n",
    "# http://www.moe.gov.cn/jyb_xwfb/gzdt_gzdt/s5987/202103/W020210329527301787356.pdf\n",
    "PDF_DISAMBIG_MP = {\n",
    "   #(level, hanzi, pinyin, pos) -> term_id\n",
    "   (1, '地', 'de', '助'):\t'L1-0066',\n",
    "   (1, '地', 'dì', '名'):\t'L1-0069',\n",
    "   (1, '干', 'gān', '形'):\t'L1-0111',\n",
    "   (1, '干', 'gàn', '动'):\t'L1-0113',\n",
    "   (1, '还', 'hái', '副'):\t'L1-0132',\n",
    "   (1, '还', 'huán', '动'):\t'L1-0153',\n",
    "   (2, '长', 'cháng', '形'):\t'L2-0055',\n",
    "   (2, '长', 'zhǎng', '动'):\t'L2-0715',\n",
    "   (2, '倒', 'dǎo', '动'):\t'L2-0109',\n",
    "   (2, '倒', 'dào', '动'):\t'L2-0111',\n",
    "   (2, '得', 'dé', '动'):\t'L2-0115',\n",
    "   (2, '得', 'de', '助'):\t'L2-0118',\n",
    "   (2, '实在', 'shízài', '副'): 'L2-0487',\n",
    "   (2, '实在', 'shízai', '形'): 'L2-0488',\n",
    "   (3, '背', 'bèi', '名'):\t'L3-0030',\n",
    "   (3, '调', 'diào', '动'):\t'L3-0153',\n",
    "   (3, '调', 'tiáo', '动'):\t'L3-0708',\n",
    "   (3, '精神', 'jīngshén', '名'): 'L3-0386',\n",
    "   (3, '精神', 'jīngshen', '形、名'): 'L3-0387',\n",
    "   (4, '倒车', 'dǎo∥chē', ''):\t'L4-0169',\n",
    "   (4, '倒车', 'dào∥chē', ''):\t'L4-0170',\n",
    "   (4, '划', 'huá', '动'):\t'L4-0328',\n",
    "   (4, '划', 'huà', '动'):\t'L4-0329',\n",
    "   (4, '卷', 'juǎn', '动'):\t'L4-0428',\n",
    "   (4, '卷', 'juàn', '量'):\t'L4-0429',\n",
    "   (4, '挑', 'tiāo', '动'):\t'L4-0711',\n",
    "   (4, '挑', 'tiǎo', '动'):\t'L4-0714',\n",
    "   (5, '编辑', 'biānjí', '动'): 'L5-0035',\n",
    "   (5, '编辑', 'biānji', '名'): 'L5-0036',\n",
    "   (5, '扇', 'shān', '动'):\t'L5-0644',\n",
    "   (5, '扇', 'shàn', '量、名'): 'L5-0645',\n",
    "   (5, '吐', 'tǔ', '动'):\t'L5-0772',\n",
    "   (5, '吐', 'tù', '动'):\t'L5-0773',\n",
    "   (6, '露', 'lòu', '动'):\t'L6-0557',\n",
    "   (6, '露', 'lù', '动'):\t'L6-0562',\n",
    "   (6, '蒙', 'mēng', '动'):\t'L6-0573',\n",
    "   (6, '蒙', 'méng', '动'):\t'L6-0574',\n",
    "   (7, '担', 'dān', '动'):\t'L7-0736',\n",
    "   (7, '担', 'dàn', '量'):\t'L7-0748',\n",
    "   (7, '大意', 'dàyì', '名'):\t'L7-0719',\n",
    "   (7, '大意', 'dàyi', '形'):\t'L7-0720',\n",
    "   (7, '地道', 'dìdào', '名'):\t'L7-0830',\n",
    "   (7, '地道', 'dìdao', '形'):\t'L7-0831',    \n",
    "   (7, '缝', 'féng', '动'):\t'L7-1175',\n",
    "   (7, '缝', 'fèng', '名'):\t'L7-1179',\n",
    "   (7, '晃', 'huǎng', '动'):\t'L7-1781',\n",
    "   (7, '晃', 'huàng', '动'):\t'L7-1784',\n",
    "   (7, '哄', 'hōng', '拟声'): 'L7-1657',\n",
    "   (7, '哄', 'hǒng', '动'):\t'L7-1671',\n",
    "   (7, '哄', 'hòng', '动'):\t'L7-1672',\n",
    "   (7, '闷', 'mēn', '形、动'): 'L7-2831',\n",
    "   (7, '闷', 'mèn', '形'):\t'L7-2836',\n",
    "   (7, '拧', 'níng', '动'):\t'L7-3014',\n",
    "   (7, '拧', 'nǐng', '动'):\t'L7-3017',\n",
    "}\n",
    "\n",
    "# Manual expansions for variants/special entries\n",
    "variants_df = pd.read_csv('data/variants.csv', dtype='str').fillna('')\n",
    "variants_mp = variants_df.groupby('ID').apply(lambda X: X.drop(columns='ID').to_dict(orient='records')).to_dict()\n",
    "for vals in variants_mp.values():  # drop NAs\n",
    "    for v in vals:\n",
    "        for col in list(v.keys()):\n",
    "            if v[col].strip() == '':\n",
    "                v.pop(col)\n",
    "\n",
    "# Normalize pinyin for matching: drop ∥· and undo tone changes to 不 一\n",
    "def normalize_pinyin(pinyin, hz):\n",
    "    mp = {'pò∥àn': \"pò'àn\", 'gǎn∥ēn': \"gǎn'ēn\", 'yìxīn-yíyì': 'yīxīn-yīyì'}\n",
    "    if pinyin in mp:\n",
    "        return mp[pinyin]\n",
    "\n",
    "    pinyin = re.sub('[∥·]', '', pinyin)\n",
    "\n",
    "    if '不' in hz and 'bú' in pinyin:\n",
    "        assert hz.count('不') == pinyin.count('bú') + pinyin.count('bù'), (pinyin, hz)\n",
    "        pinyin = pinyin.replace('bú', 'bù')\n",
    "\n",
    "    if '一' in hz and re.search('(yí|yì)', pinyin):\n",
    "        assert hz.count('一') == pinyin.count('yí') + pinyin.count('yì'), (pinyin, hz)\n",
    "        pinyin = pinyin.replace('yí', 'yī')\n",
    "        pinyin = pinyin.replace('yì', 'yī')\n",
    "\n",
    "    return pinyin\n",
    "\n",
    "\n",
    "rows = []\n",
    "seen_ids = set()\n",
    "\n",
    "for row in sh_df.sort_values(['Level', 'Pinyin']).itertuples():\n",
    "    # Determine term_id (level+index from .pdf) by matching up with pleco's list\n",
    "    hz = row.Hanzi.replace('∣', '｜').replace('1', '¹').replace('2', '²')\n",
    "    ids = (\n",
    "        level_ocr_to_id.get((row.Level, hz), []) +\n",
    "        level_ocr_to_id.get((row.Level, f'{hz}（{row.POS}）'), [])\n",
    "    )\n",
    "    if len(ids) >= 2:\n",
    "        key = (row.Level, row.Hanzi, row.Pinyin, row.POS)\n",
    "        ids = [PDF_DISAMBIG_MP[key]]\n",
    "        dis_ids=list(ids)\n",
    "        del PDF_DISAMBIG_MP[key]\n",
    "    assert len(ids) == 1\n",
    "    term_id = ids[0]\n",
    "    assert term_id in pleco_df.index\n",
    "    assert term_id not in seen_ids\n",
    "    seen_ids.add(term_id)\n",
    "\n",
    "    # Cleanup formatting in pinyin slightly\n",
    "    pinyin = row.Pinyin\n",
    "    for x, y in [(\"[’‘]\", \"'\"), (' *（', ' ('), ('） *', ') '), ('∣', '|')]:\n",
    "        pinyin = re.sub(x, y, pinyin).strip()\n",
    "    assert re.match(r\"^[-a-zāáǎàēéěèīíǐìōóǒòūúǔùüǖǘǚǜ ∥·'|/…()]+$\", pinyin.lower()), pinyin\n",
    "\n",
    "    # Translate part of speech to english\n",
    "    POS_MAP = {\n",
    "        '名': 'N',     # noun\n",
    "        '动': 'V',     # verb\n",
    "        '形': 'Adj',   # adjective/state verb; Vs in TOCFL\n",
    "        '副': 'Adv',   # adverb\n",
    "        '代': 'Pron',  # pronoun; sometimes Det/N in TOCFL\n",
    "        '数': 'Num',   # numeral\n",
    "        '量': 'M',     # measure word/classifier\n",
    "        '介': 'Prep',  # preposition\n",
    "        '连': 'Conj',  # conjunction\n",
    "        '助': 'Aux',   # auxiliary word; usually Ptc in TOCFL\n",
    "        '叹': 'Intj',  # interjection/exclamation/particle 喂 啊 哎呀, Ptc in tocfl\n",
    "        '前缀': 'Prefix',\n",
    "        '后缀': 'Suffix',\n",
    "        '拟声': 'Phonetic',\n",
    "    }\n",
    "    pos = ''\n",
    "    if row.POS:\n",
    "        pos = '/'.join([POS_MAP[s] for s in row.POS.split('、')])\n",
    "\n",
    "    hanzi = re.sub(r'[\\u2223∣|]', '|', row.Hanzi)\n",
    "    assert re.match(r'^([\\u4E00-\\u9FFF]|[〇|…（）12])+$', hanzi)\n",
    "\n",
    "    variants = variants_mp.get(term_id, [])\n",
    "    if any(c in '12（）|…/'  for c in (hanzi+pinyin)):\n",
    "        # all special entries have Variants entry\n",
    "        assert term_id in variants_mp, row\n",
    "    else:\n",
    "        assert term_id not in variants_mp, row\n",
    "\n",
    "    for variant in variants:\n",
    "        variant['Pinyin'] = normalize_pinyin(variant['Pinyin'], variant['Simplified'])\n",
    "\n",
    "    rows.append({\n",
    "        'ID': term_id,\n",
    "        'Simplified': hanzi,\n",
    "        'Pinyin': normalize_pinyin(pinyin, hanzi),\n",
    "        'POS': pos,\n",
    "        'Level': str(row.Level) if row.Level <= 6 else '7-9',\n",
    "        'WebNo': row.No,\n",
    "        'WebPinyin': pinyin,\n",
    "        'OCR': pleco_df.loc[ids[0], 'OCR'],\n",
    "        'Variants': json.dumps(variants_mp[term_id], ensure_ascii=False) if term_id in variants_mp else '',\n",
    "    })\n",
    "\n",
    "hsk30_df = pd.DataFrame(rows).sort_values('ID').reset_index(drop=True).set_index('ID').copy()\n",
    "hsk30_df.to_csv('hsk30.csv')\n",
    "\n",
    "assert len(PDF_DISAMBIG_MP) == 0  # all disambiguation entries matched\n",
    "assert len(hsk30_df) == 11092\n",
    "assert list(hsk30_df.index) == list(pleco_df.index)\n",
    "assert list(hsk30_df.Level.value_counts().sort_index()) == [500, 772, 973, 1000, 1071, 1140, 5636]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eaf10b-4594-48c0-8e7b-7ac57a563be0",
   "metadata": {},
   "source": [
    "## Add traditional forms and join with CEDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f90be4-c864-4523-aab2-0821c81e691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing:\tL1-0044 车上 chē shang  ['車上']\n",
      "Multiple:\tL1-0213,里,lǐ,N: 裡|里[li3]/裏|里[li3]\n",
      "Multiple:\tL1-0251,哪里,nǎlǐ,Pron: 哪裡|哪里[na3 li3]/哪裏|哪里[na3 li3]\n",
      "Multiple:\tL1-0256,那里,nàlǐ,Pron: 那裡|那里[na4 li5]/那裏|那里[na4 li5]\n",
      "Multiple:\tL2-0028,表,biǎo,N: 表|表[biao3]/錶|表[biao3]\n",
      "Missing:\tL2-0034 不太 bù tài  ['不太']\n",
      "Missing:\tL2-0044 不一会儿 bù yīhuìr  ['不一會兒']\n",
      "Missing:\tL2-0264 见过 jiànguo  ['見過']\n",
      "Missing:\tL2-0507 送到 sòngdào  ['送到']\n",
      "Missing:\tL2-0722 这时候 zhè shíhou  ['這時候']\n",
      "Missing:\tL3-0192 放到 fàngdào  ['放到']\n",
      "Missing:\tL3-0508 能不能 néng bu néng  ['能不能']\n",
      "Multiple:\tL3-0693,台,tái,N/M: 台|台[tai2]/臺|台[tai2]\n",
      "Multiple:\tL4-0099,冲,chōng,V: 衝|冲[chong1]/沖|冲[chong1]\n",
      "Multiple:\tL4-0328,划,huá,V: 划|划[hua2]/劃|划[hua2]\n",
      "Multiple:\tL4-0337,汇报,huìbào,V/N: 匯報|汇报[hui4 bao4]/彙報|汇报[hui4 bao4]\n",
      "Multiple:\tL4-0428,卷,juǎn,V: 卷|卷[juan3]/捲|卷[juan3]\n",
      "Multiple:\tL4-0482,了解,liǎojiě,V: 了解|了解[liao3 jie3]/瞭解|了解[liao3 jie3]\n",
      "Missing:\tL4-0853 眼里 yǎnli N ['眼裡']\n",
      "Missing:\tL4-0897 有劲儿 yǒujìnr  ['有勁兒']\n",
      "Multiple:\tL5-0094,尝,cháng,V: 嚐|尝[chang2]/嘗|尝[chang2]\n",
      "Missing:\tL5-0108 城里 chénglǐ N ['城裡']\n",
      "Multiple:\tL5-0224,发布,fābù,V: 發布|发布[fa1 bu4]/發佈|发布[fa1 bu4]\n",
      "Simp diff:\tL5-0740,抬,tái,V: 抬|抬[tai2]; our 抬, tw2s 擡; s2tw 抬\n",
      "Simp diff:\tL5-0741,抬头,táitóu,: 抬頭|抬头[tai2 tou2]; our 抬頭, tw2s 擡头; s2tw 抬頭\n",
      "Multiple:\tL6-0042,编制,biānzhì,V: 編制|编制[bian1 zhi4]/編製|编制[bian1 zhi4]\n",
      "Multiple:\tL6-0340,刮,guā,V: 刮|刮[gua1]/颳|刮[gua1]\n",
      "Missing:\tL6-0376 很难说 hěn nánshuō  ['很難說']\n",
      "Multiple:\tL6-0829,托,tuō,V: 托|托[tuo1]/託|托[tuo1]\n",
      "Multiple:\tL6-0912,凶,xiōng,Adj: 凶|凶[xiong1]/兇|凶[xiong1]\n",
      "Missing:\tL6-0957 一番 yīfān  ['一番']\n",
      "Multiple:\tL6-0996,于,yú,Prep: 于|于[yu2]/於|于[yu2]\n",
      "Multiple:\tL6-1030,扎,zhā,V: 扎|扎[zha1]/紮|扎[zha1]\n",
      "Missing:\tL6-1079 指着 zhǐzhe  ['指著']\n",
      "Missing:\tL7-0253 不利于 bùlì yú  ['不利於']\n",
      "Missing:\tL7-0283 不肯 bù kěn  ['不肯']\n",
      "Missing:\tL7-0286 不难 bù nán  ['不難']\n",
      "Missing:\tL7-0290 不如说 bùrú shuō  ['不如說']\n",
      "Missing:\tL7-0300 不予 bù yǔ  ['不予']\n",
      "Multiple:\tL7-0383,铲,chǎn,V: 剷|铲[chan3]/鏟|铲[chan3]\n",
      "Missing:\tL7-0443 趁着 chènzhe  ['趁著']\n",
      "Multiple:\tL7-0476,痴呆,chīdāi,Adj: 痴獃|痴呆[chi1 dai1]/癡呆|痴呆[chi1 dai1]\n",
      "Multiple:\tL7-0767,荡漾,dàngyàng,V: 盪漾|荡漾[dang4 yang4]/蕩漾|荡漾[dang4 yang4]\n",
      "Missing:\tL7-0896 定为 dìngwéi  ['定為']\n",
      "Multiple:\tL7-1013,发布会,fābùhuì,N: 發佈會|发布会[fa1 bu4 hui4]/發布會|发布会[fa1 bu4 hui4]\n",
      "Multiple:\tL7-1036,帆,fān,N: 帆|帆[fan1]/颿|帆[fan1]\n",
      "Multiple:\tL7-1081,仿,fǎng,V: 仿|仿[fang3]/彷|仿[fang3]\n",
      "Missing:\tL7-1092 飞往 fēiwǎng  ['飛往']\n",
      "Multiple:\tL7-1218,复合,fùhé,V: 復合|复合[fu4 he2]/複合|复合[fu4 he2]\n",
      "Missing:\tL7-1362 公益性 gōngyìxìng N ['公益性']\n",
      "Multiple:\tL7-1704,糊,hú,V: 糊|糊[hu2]/餬|糊[hu2]\n",
      "Missing:\tL7-1743 怀着 huáizhe  ['懷著']\n",
      "Missing:\tL7-2561 离谱儿 lípǔr  ['離譜兒']\n",
      "Multiple:\tL7-2768,马后炮,mǎhòupào,N: 馬後炮|马后炮[ma3 hou4 pao4]/馬後砲|马后炮[ma3 hou4 pao4]\n",
      "Missing:\tL7-2972 难以想象 nányǐ-xiǎngxiàng  ['難以想象']\n",
      "Missing:\tL7-3893 说起来 shuō qǐlái  ['說起來']\n",
      "Missing:\tL7-4577 效仿 xiàofǎng V ['效仿']\n",
      "Multiple:\tL7-4660,凶残,xiōngcán,Adj: 兇殘|凶残[xiong1 can2]/凶殘|凶残[xiong1 can2]\n",
      "Multiple:\tL7-5091,余,yú,V/Num: 余|余[yu2]/餘|余[yu2]\n",
      "Multiple:\tL7-5194,暂,zàn,Adv: 暫|暂[zan4]/蹔|暂[zan4]\n",
      "Multiple:\tL7-5321,征,zhēng,V: 征|征[zheng1]/徵|征[zheng1]\n",
      "Missing:\tL7-5399 致力于 zhìlì yú  ['致力於']\n",
      "Multiple:\tL7-5471,注,zhù,N/V: 注|注[zhu4]/註|注[zhu4]\n",
      "Simp diff:\tL7-5487,专著,zhuānzhù,N: 專著|专著[zhuan1 zhu4]; our 專著, tw2s 专着; s2tw 專著\n",
      "Missing:\tL7-5538 着眼于 zhuóyǎn yú  ['著眼於']\n"
     ]
    }
   ],
   "source": [
    "# ID, simplified => space separated trad variants. Main/TW variant first.\n",
    "TRAD_VARIANTS_MP = {\n",
    "  ('L1-0213', '里'): '裡 裏',\t    # lǐ N\n",
    "  ('L1-0251', '哪里'): '哪裡 哪裏',\t# nǎlǐ Pron\n",
    "  ('L1-0256', '那里'): '那裡 那裏',\t# nàlǐ Pron\n",
    "  ('L1-0271', '你'): '你',\t        # nǐ Pron, TW has 妳 but that's rather esoteric for HSK\n",
    "  ('L2-0028', '表'): '表 錶',\t    # biǎo\n",
    "  ('L2-0377', '面'): '面',\t        # miàn N/M\n",
    "  ('L2-0378', '面'): '麵',\t        # miàn N\n",
    "  ('L2-0574', '喂'): '喂',\t        # wèi Intj\n",
    "  ('L3-0693', '台'): '台 臺',\t    # tái N/M\n",
    "  ('L2-0742', '周'): '週',\t        # zhōu M\n",
    "  ('L3-0772', '系'): '系',\t        # xì N\n",
    "  ('L3-0898', '证'): '證',\t        # zhèng N\n",
    "  ('L3-0954', '准'): '準',\t        # zhǔn Adj/Adv\n",
    "  ('L4-0099', '冲'): '衝 沖',\t    # chōng V\n",
    "  ('L4-0430', '卷'): '捲 卷',\t    # juǎn V\n",
    "  ('L4-0482', '了解'): '了解 瞭解',\t# liǎojiě V\n",
    "  ('L4-0683', '松'): '鬆',\t        # sōng Adj/V\n",
    "  ('L4-0760', '喂'): '餵',\t        # wèi V\n",
    "  ('L5-0094', '尝'): '嚐 嘗',\t    # cháng V\n",
    "  ('L5-0120', '丑'): '醜',\t        # chǒu Adj\n",
    "  ('L5-0224', '发布'): '發布 發佈',\t# fābù V\n",
    "  ('L5-0326', '胡同儿'): '胡同兒',\t# hútòngr N, theoretical variant: 衚衕兒 but taiwanese wouldn't write with erhua\n",
    "  ('L5-0406', '尽可能'): '盡可能',\t# jìn kěnéng; alt. 儘可能 has different tone [jǐn]\n",
    "  ('L7-3998', '坛'): '壇',\t        # tán N; alt. 罈 esoteric\n",
    "  ('L7-4684', '须'): '須',\t        # xū V; alt. 鬚 beard\n",
    "\n",
    "  # complicated/rare variant cases, leave unchanged as main trad variant\n",
    "  ('L7-4577', '效仿'): '效仿',\n",
    "  ('L7-4741', '熏'): '熏',\n",
    "  ('L7-4742', '熏陶'): '熏陶 薰陶',\n",
    "}\n",
    "\n",
    "UNTONE_MP = {\n",
    "    'a': 'a', 'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "    'e': 'e', 'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e',\n",
    "    'o': 'o', 'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o',\n",
    "    'i': 'i', 'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "    'u': 'u', 'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u',\n",
    "    'ü': 'ü', 'ǖ': 'ü', 'ǘ': 'ü', 'ǚ': 'ü', 'ǜ': 'ü'\n",
    "}\n",
    "\n",
    "opencc_tw2s = opencc.OpenCC('tw2s')\n",
    "opencc_s2tw = opencc.OpenCC('s2tw')\n",
    "\n",
    "# At this point, additional data sources are needed from zhongwen repo\n",
    "# to cross-reference against -- run the notebook from inside that repo.\n",
    "cedict_df = pd.read_csv('../cedict/cedict.csv', dtype='str').fillna('')\n",
    "cedict_idx = cedict_df.assign(idx=cedict_df.index).groupby('Simplified').idx.apply(list)\n",
    "\n",
    "# Characters from Table of General Standard Chinese Characters for verification.\n",
    "# HSK uses characters from only first two levels\n",
    "tgh_chars = set(pd.read_csv('../chars/tgh.csv')[lambda X: X.level <= 2].char)\n",
    "assert len(tgh_chars) == 6500\n",
    "\n",
    "tw_words = \\\n",
    "  set(pd.read_csv('../dangdai/dangdai.csv').Traditional) | \\\n",
    "  set(pd.read_csv('../modernchinese/modernchinese.csv').Traditional) | \\\n",
    "  set(pd.read_csv('../pavc/pavc.csv').Traditional) | \\\n",
    "  set(pd.read_csv('../tocfl/tocfl-expanded.csv').Traditional) | \\\n",
    "  set(pd.read_csv('../tbcl/tbcl-expanded.csv').Traditional)\n",
    "\n",
    "# Check if pinyin from hsk (py1) matches cedict's (py2)\n",
    "# Optionally matching untoned vowels with tones if untone==True.\n",
    "def pinyin_matches(py1, py2, hz='', untone=False, yi=True, bu=True):  # py2 cedict\n",
    "    py1 = py1.lower()\n",
    "    py2 = py2.lower()\n",
    "    i, j = 0, 0\n",
    "    while i < len(py1) or j < len(py2):\n",
    "        a = ''\n",
    "        if i < len(py1):\n",
    "            a = py1[i]\n",
    "            if a in \"-∥·', \":\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        b = ''\n",
    "        if j < len(py2):\n",
    "            b = py2[j]\n",
    "            if b in \"-∥·', \":\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "        match = (a == b)\n",
    "        match |= untone and (UNTONE_MP.get(a, a) == b or a == UNTONE_MP.get(b, b))\n",
    "        if i > 0 and j > 0:\n",
    "            match |= yi and py1[i-1:i+1] in ['yí', 'yì'] and py2[j-1:j+1] == 'yī' and '一' in hz\n",
    "            match |= bu and py1[i-1:i+1] == 'bú' and py2[j-1:j+1] == 'bù' and '不' in hz\n",
    "\n",
    "        if match:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    return i == len(py1) and j == len(py2)\n",
    "\n",
    "# Join with CEDICT and determine traditional form.\n",
    "# Updates row in place: adds 'Traditional' and 'CEDICT' columns.\n",
    "def cedict_join(row):\n",
    "    matches = [cedict_df.iloc[i] for i in cedict_idx.get(row['Simplified'], [])]\n",
    "    trad_variants = TRAD_VARIANTS_MP.get((row['ID'], row['Simplified']), '').split()\n",
    "\n",
    "    if trad_variants:\n",
    "        matches = [m for m in matches if m.Traditional in trad_variants]\n",
    "        matches.sort(key=lambda m: trad_variants.index(m.Traditional))\n",
    "        # Keep applying other filters, at least pinyin filter is needed for wei4\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        if not trad_variants:\n",
    "            trad_variants = [opencc.OpenCC('s2tw').convert(row['Simplified'])]\n",
    "        if row['Simplified'] != opencc_tw2s.convert(trad_variants[0]):\n",
    "            print(f\"{row['ID']},{row['Simplified']},{row['Pinyin']},{row['POS']}: {matches_str};  {opencc_tw2s.convert(trad_variants[0])}\")\n",
    "        if row.get('Example') != '1':\n",
    "            print(f\"Missing:\\t{row['ID']} {row['Simplified']} {row['Pinyin']} {row['POS']} {trad_variants}\")\n",
    "        row['Traditional'] = '|'.join(trad_variants)\n",
    "        row['CEDICT'] = ''\n",
    "        return\n",
    "\n",
    "    if len(matches) >= 2:\n",
    "        filt = [m for m in matches if pinyin_matches(row['Pinyin'], m.Pinyin, row['Simplified'])]\n",
    "        if len(filt) == 0:\n",
    "            filt = [m for m in matches if pinyin_matches(row['Pinyin'], m.Pinyin, row['Simplified'], untone=True)]\n",
    "        if len(filt) == 0:\n",
    "            mm = [m.to_dict() for m in matches]\n",
    "            print(f\"No matching pinyin for: {row['ID']} {row['Simplified']} {row['Pinyin']}: {mm}\")\n",
    "        else:\n",
    "            matches = filt\n",
    "\n",
    "    # Filter name matches by capitalization - CEDICT has a lot of names\n",
    "    if len(matches) >= 2:\n",
    "        filt = [m for m in matches if m.Pinyin[0].islower() == row['Pinyin'][0].islower()]\n",
    "        assert len(filt) > 0\n",
    "        matches = filt\n",
    "\n",
    "    # Filter by words contained in datasets for traditional chinese learners.\n",
    "    # TODO: further verification, add MOE dict\n",
    "    if len(matches) >= 2 and not trad_variants:\n",
    "        filt = [m for m in matches if m.Traditional in tw_words]\n",
    "        if len(filt) > 0:\n",
    "            matches = filt\n",
    "\n",
    "    # Filter weird/obsolete char matches. TODO: further verification\n",
    "    if len(matches) >= 2 and not trad_variants:\n",
    "        filt = [m for m in matches if not re.match('^((old|archaic|Japanese)? ?variant of).*', m.Definitions)]\n",
    "        assert len(filt) > 0, row\n",
    "        matches = filt\n",
    "\n",
    "    # Put opencc's s2tw conversion at the front.\n",
    "    if len(matches) >= 2 and not trad_variants:\n",
    "        matches.sort(key=lambda m: (int(m.Traditional != opencc_tw2s.convert(m.Simplified)), m.Traditional))\n",
    "\n",
    "    if not trad_variants:\n",
    "        trad_variants = [m.Traditional for m in matches]\n",
    "\n",
    "    row['CEDICT'] = '/'.join([f'{m.Traditional}|{m.Simplified}[{m.PinyinNumbered}]' for m in matches])\n",
    "    row['Traditional'] = '|'.join(trad_variants)\n",
    "\n",
    "    if len(matches) >= 2 and len(set(m.Traditional for m in matches)) > 1:\n",
    "        print(f\"Multiple:\\t{row['ID']},{row['Simplified']},{row['Pinyin']},{row['POS']}: {row['CEDICT']}\")\n",
    "    if row['Simplified'] != opencc_tw2s.convert(trad_variants[0]):\n",
    "        # simplified back translation diff\n",
    "        print(f\"Simp diff:\\t{row['ID']},{row['Simplified']},{row['Pinyin']},{row['POS']}:\",\n",
    "              f\"{row['CEDICT']}; our {row['Traditional']}, tw2s {opencc_tw2s.convert(trad_variants[0])}; \"\n",
    "              f\"s2tw {opencc_s2tw.convert(row['Simplified'])}\")\n",
    "\n",
    "\n",
    "df = hsk30_df.copy()\n",
    "df.insert(1, 'Traditional', '')\n",
    "df['CEDICT'] = ''\n",
    "\n",
    "for row in df.reset_index().fillna('').to_dict(orient='records'):\n",
    "    if row['Variants']:\n",
    "        variants = json.loads(row['Variants'])\n",
    "        for variant in variants:\n",
    "            vrow = dict(row)\n",
    "            vrow.update(variant)\n",
    "            cedict_join(vrow)\n",
    "            variant['Traditional'] = vrow['Traditional']\n",
    "            variant['CEDICT'] = vrow['CEDICT']\n",
    "        df.loc[row['ID'], 'Variants'] = json.dumps(variants, ensure_ascii=False)\n",
    "        df.loc[row['ID'], 'Traditional'] = opencc_s2tw.convert(row['Simplified'])\n",
    "        df.loc[row['ID'], 'CEDICT'] = [v['CEDICT'] for v in variants if 'CEDICT' in v][0]\n",
    "        assert all(c in tgh_chars or c in '|（）〇12…' for c in row['Simplified']), row\n",
    "    else:\n",
    "        cedict_join(row)\n",
    "        df.loc[row['ID'], 'Traditional'] = row['Traditional']\n",
    "        df.loc[row['ID'], 'CEDICT'] = row['CEDICT']\n",
    "        assert all(c in tgh_chars for c in row['Simplified']), row\n",
    "\n",
    "hsk30_trad_df = df\n",
    "hsk30_trad_df.to_csv('hsk30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64376a5-6bee-42fa-993f-fd86c7079335",
   "metadata": {},
   "source": [
    "## Expand variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142e5b9-f4d9-4802-80b9-372f8ac85efa",
   "metadata": {},
   "source": [
    "Expand both simplified and traditional variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c7be03-6f67-40e6-9fb1-ef5686eed914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsk30-expanded.csv: 11165 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expanded_rows = []\n",
    "for row in hsk30_trad_df.reset_index().to_dict(orient='records'):\n",
    "    variants = row.pop('Variants')\n",
    "    variants = json.loads(variants) if variants else [{}]\n",
    "    for variant in variants:  # expand simplified\n",
    "        vrow = dict(row)\n",
    "        vrow.update(variant)\n",
    "        assert len(vrow['Traditional'].split()) == 1  # not empty and not spaces\n",
    "        assert '/' not in vrow['Traditional']\n",
    "        trad_variants = vrow['Traditional'].split('|')\n",
    "\n",
    "        for trad in trad_variants:  # expand traditional\n",
    "            vrow = dict(row)\n",
    "            vrow.update(variant)\n",
    "            vrow['Traditional'] = trad\n",
    "            if '/' in vrow['CEDICT']:\n",
    "                vrow['CEDICT'] = [v for v in vrow['CEDICT'].split('/') if v.startswith(trad + '|')][0]\n",
    "            expanded_rows.append(vrow)\n",
    "\n",
    "            for col in ['Simplified', 'Traditional', 'Pinyin']:\n",
    "                assert all(c not in '|/…()（）12∥·' for c in vrow[col]), vrow\n",
    "            assert all(c in tgh_chars or c == '〇' for c in vrow['Simplified']), vrow\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df.to_csv('hsk30-expanded.csv', index=False)\n",
    "print('hsk30-expanded.csv: %d rows\\n' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bea6f-6993-420b-90f8-4b6d2d5be4fc",
   "metadata": {},
   "source": [
    "## Character list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657b5c2b-2a48-4521-bd7d-56bf76642a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In wordlist, but not char list: {'〇'}\n",
      "Extra 29 chars not in wordlist: 吴冯州渝袁粤赵蜀秦潘韩浙杭欧邓宋澳淮魏浦郭孟孔洲吕沪刘曹唐\n",
      "Final list: 3000 chars\n",
      "Level\n",
      "1       300\n",
      "2       300\n",
      "3       300\n",
      "4       300\n",
      "5       300\n",
      "6       300\n",
      "7-9    1200\n",
      "Name: count, dtype: int64\n",
      "WritingLevel\n",
      "     1800\n",
      "1     300\n",
      "2     400\n",
      "3     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "char_level = {}\n",
    "char_to_words = {}\n",
    "char_to_ids = {}\n",
    "char_to_trad = {}\n",
    "for row in expanded_df.itertuples():\n",
    "    lv = int(row.Level[0])\n",
    "    assert len(row.Simplified) == len(row.Traditional)\n",
    "    for ch, tch in zip(row.Simplified, row.Traditional):\n",
    "        char_level[ch] = min(char_level.get(ch, lv), lv)\n",
    "        wl = char_to_words.setdefault(ch, [])\n",
    "        if row.Simplified not in wl:\n",
    "            wl.append(row.Simplified)\n",
    "        char_to_ids.setdefault(ch, set()).add(row.ID)\n",
    "        char_to_trad.setdefault(ch, [])\n",
    "        if tch not in char_to_trad[ch]:\n",
    "            char_to_trad[ch].append(tch)\n",
    "\n",
    "df = pd.read_csv('downloads/shawkynasr-hanzi.csv', encoding='GB2312').rename(columns={\n",
    "    'No.': 'No',\n",
    "    '汉字': 'Hanzi',\n",
    "    '级别': 'Level',\n",
    "    '拼音': 'Pinyin',\n",
    "})\n",
    "df['Level'] = df['Level'].map({'一级':1, '三级':3, '二级':2, '五级':5, '六级':6, '四级':4, '高等':7})\n",
    "\n",
    "miss_chars = set(char_level.keys()) - set(df.Hanzi)\n",
    "print('In wordlist, but not char list:', miss_chars)\n",
    "assert miss_chars == {'〇'}  # variant outside unihan anyway\n",
    "\n",
    "extra_chars = set(df.Hanzi) - set(char_level.keys())\n",
    "print(f'Extra {len(extra_chars)} chars not in wordlist: {\"\".join(extra_chars)}')\n",
    "assert set(df[df.Hanzi.isin(extra_chars)].Level) == {7}  # all are L7 chars\n",
    "\n",
    "for c in extra_chars:\n",
    "    char_level[c] = 7\n",
    "del char_level['〇']\n",
    "print('Final list: %d chars' % len(char_level))\n",
    "assert set(char_level.keys()) == set(df.Hanzi)\n",
    "\n",
    "# Levels don't match\n",
    "#for row in df.itertuples():\n",
    "#    if char_level[row.Hanzi] != row.Level:\n",
    "#        print(row.Hanzi, char_level[row.Hanzi], row.Level)\n",
    "\n",
    "char_writing_level = {}\n",
    "char_level_index = {}\n",
    "lv = 0\n",
    "for line in open('downloads/pleco-charlist.txt'):\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith('#'):\n",
    "        continue\n",
    "    if '级汉字表' in line:\n",
    "        lv += 1\n",
    "        idx = 0\n",
    "    elif '等手写字表' in line:\n",
    "        lv = (lv + 10) - (lv % 10)\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx += 1\n",
    "        m = re.match('^([0-9]+)\\t([\\u4300-\\u9FFF])$', line)\n",
    "        assert m, line\n",
    "        assert int(m[1]) == idx\n",
    "        hanzi = m[2]\n",
    "        hanzi = {'洎': '泊'}.get(hanzi, hanzi)  # OCR error https://github.com/elkmovie/hsk30/issues/9\n",
    "        if lv >= 10:\n",
    "            assert hanzi in char_level\n",
    "            char_writing_level[hanzi] = lv//10\n",
    "        else:\n",
    "            assert char_level[hanzi] == lv, (hanzi, char_level[hanzi], lv)\n",
    "            char_level_index[hanzi] = (lv, idx)\n",
    "\n",
    "rows = []\n",
    "for ch in sorted(char_level.keys(), key=lambda ch: char_level_index[ch]):\n",
    "    assert len(ch) == 1\n",
    "    examples = char_to_words.get(ch, [])\n",
    "    rows.append({\n",
    "        'Hanzi': ch,\n",
    "        'Level': '7-9' if str(char_level[ch]) == '7' else str(char_level[ch]),\n",
    "        'WritingLevel': str(char_writing_level.get(ch, '')),\n",
    "        'Traditional': '/'.join(char_to_trad.get(ch, [])),\n",
    "        #'Freq': len(char_to_ids.get(ch, [])),\n",
    "        'Freq': len(char_to_words.get(ch, [])),\n",
    "        'Examples': ' '.join(examples[:min(len(examples), 5)]),\n",
    "    })\n",
    "chars_df = pd.DataFrame(rows).set_index('Hanzi')\n",
    "chars_df.to_csv('hsk30-chars.csv')\n",
    "\n",
    "print(chars_df.Level.value_counts().sort_index())\n",
    "print(chars_df.WritingLevel.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadceca-d24b-4f8a-804f-bfa838e10487",
   "metadata": {},
   "source": [
    "## Check readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab425ec0-e198-4804-8523-b5f29e286e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['困难', '困難', 'kùnnan'] vs. ['kùn nán', 'kùn-nàn', 'kùn nàn', 'kùnnán', 'kùnnàn', 'kùn-nán']\n",
      "['学问', '學問', 'xuéwen'] vs. ['xué wèn', 'xué-wèn', 'xuéwèn']\n",
      "['买卖', '買賣', 'mǎimai'] vs. ['mǎimài', 'mǎi-mài', 'mǎi mài']\n",
      "['熟悉', '熟悉', 'shúxi'] vs. ['shóu xī', 'shóuxī', 'shóu-xī', 'shúxī', 'shú-xī', 'shú xī']\n",
      "['队伍', '隊伍', 'duìwu'] vs. ['duìwǔ', 'duì wǔ', 'duì-wǔ']\n",
      "['比试', '比試', 'bǐshi'] vs. ['bíshì', 'bī-shì', 'bǐ-shì', 'bìshì', 'bí-shì', 'bǐshì', 'bì-shì', 'bī shì', 'bí shì', 'bǐ shì']\n",
      "['大大咧咧', '大大咧咧', 'dàdaliēliē'] vs. ['dàidài liěliě', 'dàdá lie liē', 'dà-dài lie liě', 'dà-dàiliě liē', 'dádà-lieliě', 'dài dà-liělie', 'dài-dà liē-lie', 'dádà lie-lie', 'dá-dàiliē-liě', 'dài-dài liěliě']\n",
      "['灯笼', '燈籠', 'dēnglong'] vs. ['dēnglóng', 'dēng-lǒng', 'dēng-lóng', 'dēnglǒng', 'dēng lóng', 'dēng lǒng']\n",
      "['动静', '動靜', 'dòngjing'] vs. ['dong jìng', 'dòng-jìng', 'dòngjìng', 'dòng jìng', 'dongjìng', 'dong-jìng']\n",
      "['风筝', '風箏', 'fēngzheng'] vs. ['fēngzhēng', 'fēng zhēng', 'fēng-zhēng']\n",
      "['固执', '固執', 'gùzhi'] vs. ['gù zhí', 'gù-zhí', 'gùzhí']\n",
      "['能耐', '能耐', 'néngnai'] vs. ['néng nài', 'néng-nài', 'néngnài']\n",
      "['盘算', '盤算', 'pánsuan'] vs. ['pán suàn', 'pánsuàn', 'pán-suàn']\n",
      "['试探', '試探', 'shìtan'] vs. ['shì-tàn', 'shìtàn', 'shì tàn']\n",
      "['硬朗', '硬朗', 'yìnglang'] vs. ['yìng-lǎng', 'yìng lǎng', 'yìnglǎng']\n",
      "['状元', '狀元', 'zhuàngyuan'] vs. ['zhuàng yuán', 'zhuàng-yuán', 'zhuàngyuán']\n",
      "['祖宗', '祖宗', 'zǔzong'] vs. ['zǔ zōng', 'zǔzōng', 'zǔ-zōng']\n"
     ]
    }
   ],
   "source": [
    "# Check pinyin against possible syllable readings in cedict\n",
    "# Should have just minor 5th tone mismatches, other diffs to look into\n",
    "if os.path.exists('../cedict/syllables.csv'):\n",
    "    readings_mp = {} #{'一': set(['yì','yí']), '不': set(['bú'])}\n",
    "    syll_df = pd.read_csv('../cedict/syllables.csv', dtype='str').fillna('')\n",
    "    for row in syll_df.itertuples():\n",
    "        readings_mp.setdefault(row.Traditional, set()).add(row.Pinyin.lower())\n",
    "    readings_mp = {x: set([y.strip().lower() for y in readings_mp[x] if y.strip()]) for x in readings_mp}\n",
    "\n",
    "    def gen_readings(trad):\n",
    "        if trad == '':\n",
    "            yield ''\n",
    "        elif trad[0] not in readings_mp or ord(trad[0]) < 0x3000:\n",
    "            yield from gen_readings(trad[1:])\n",
    "        else:\n",
    "            for x in readings_mp[trad[0]]:\n",
    "                for y in gen_readings(trad[1:]):\n",
    "                    yield x + (\"'\" if y and y[0] in 'aāáǎàeēéěèoōóǒò' else '') + y\n",
    "                    yield x + (\"-\" if y and y[0] in 'aāáǎàeēéěèoōóǒò' else '') + y\n",
    "                    if y:\n",
    "                        yield x + ' ' + y\n",
    "                        yield x + '-' + y\n",
    "\n",
    "    for row in expanded_df.fillna('').itertuples():\n",
    "        trad, pinyin = row.Traditional, row.Pinyin\n",
    "        readings = list(set(gen_readings(trad)))\n",
    "        if pinyin.lower() not in readings:\n",
    "            print(list(row._asdict().values())[2:5], 'vs.', readings[:min(len(readings), 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7dd271-3275-4e85-afa0-3b98b750ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all characters were converted to traditional\n",
    "\n",
    "unihan_df = pd.read_csv('../unihan/unihan.csv', dtype='str').fillna('').set_index('char')\n",
    "\n",
    "# https://www.unicode.org/reports/tr38/#SCTC\n",
    "def classify(c):\n",
    "    if c == '〇': return 'both'\n",
    "    assert c in unihan_df.index, c\n",
    "    tv = unihan_df.kTraditionalVariant.loc[c]\n",
    "    sv = unihan_df.kSimplifiedVariant.loc[c]\n",
    "    if tv == '' and sv == '': return 'both'\n",
    "    if tv == '': return 'T'\n",
    "    if sv == '': return 'S'\n",
    "    return 'complex'\n",
    "\n",
    "for row in expanded_df.itertuples():\n",
    "    trad, simp = row.Traditional, row.Simplified\n",
    "    assert len(trad) == len(simp), (row._asdict())\n",
    "    for sc, tc in zip(simp, trad):\n",
    "        assert classify(tc) in ('T', 'both', 'complex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
